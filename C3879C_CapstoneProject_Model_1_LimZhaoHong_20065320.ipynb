{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RP-SDAAI/Capstone-Project/blob/main/C3879C_CapstoneProject_Model_1_LimZhaoHong_20065320.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Student Name: Lim Zhao Hong Student ID: 20065320\n",
        "##C3879C Capstone Project Model 1: Roboflow"
      ],
      "metadata": {
        "id": "3YgC0QASjc4h"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GD9gUQpaBxNa"
      },
      "source": [
        "To train our detector we take the following steps:\n",
        "\n",
        "* Install YOLOv5 dependencies\n",
        "* Download YOLOv5 object detection datasets and meta data\n",
        "* Write our YOLOv5 Training configuration\n",
        "* Run YOLOv5 training\n",
        "* Evaluate YOLOv5 performance\n",
        "* Visualize YOLOv5 training data\n",
        "* Run YOLOv5 inference on test images\n",
        "* Export saved YOLOv5 weights for future inference\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mGmQbAO5pQb"
      },
      "source": [
        "#Install Dependencies\n",
        "\n",
        "_(Remember to choose GPU in Runtime if not already selected. Runtime --> Change Runtime Type --> Hardware accelerator --> GPU)_"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "VfUQsjYnRfUE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8e1c616-da3a-4f48-e466-142327db5cc8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "from time import sleep\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "7XhSaJ5ebcto"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbvMlHd_QwMG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "160fbf17-3989-4d22-ecb3-d7ab4b3e1180"
      },
      "source": [
        "!git clone https://github.com/ultralytics/yolov5  # clone repo\n",
        "!pip install -qr yolov5/requirements.txt  # install dependencies (ignore errors)\n",
        "%cd yolov5\n",
        "\n",
        "\n",
        "# Then, we can take a look at our training environment provided to us for free from Google Colab.\n",
        "import torch\n",
        "from IPython.display import Image, clear_output  # to display images\n",
        "#from utils.google_utils import gdrive_download  # to download models/datasets\n",
        "\n",
        "clear_output()\n",
        "print('Setup complete. Using torch %s %s' % (torch.__version__, torch.cuda.get_device_properties(0) if torch.cuda.is_available() else 'CPU'))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete. Using torch 2.0.0+cu118 _CudaDeviceProperties(name='Tesla T4', major=7, minor=5, total_memory=15101MB, multi_processor_count=40)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDIhrBF0sPaM"
      },
      "source": [
        "# Download Correctly Formatted Dataset \n",
        "\n",
        "We'll download our dataset from Roboflow. Use the \"**YOLOv5 PyTorch**\" export format. Note that the Ultralytics implementation calls for a YAML file defining where your training and test data is."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Knxi2ncxWffW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "607c142c-ba3e-44e6-efd3-9907004f6b35"
      },
      "source": [
        "# Export code snippet and paste here\n",
        "%cd /content\n",
        "\n",
        "#reference website https://app.roboflow.com/applied-artificial-intelligence/hard-hat-sample-eoeka/2\n",
        "!curl -L \"https://app.roboflow.com/ds/KuhXAyi8lq?key=WAK4ZYYsFZ\" > roboflow.zip; unzip roboflow.zip; rm roboflow.zip"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   901  100   901    0     0   1432      0 --:--:-- --:--:-- --:--:--  1432\n",
            "100 6236k  100 6236k    0     0  2741k      0  0:00:02  0:00:02 --:--:-- 4321k\n",
            "Archive:  roboflow.zip\n",
            " extracting: README.roboflow.txt     \n",
            " extracting: data.yaml               \n",
            "   creating: test/\n",
            "   creating: test/images/\n",
            " extracting: test/images/000008_jpg.rf.mR8kVxlPQ0Cc2xInZzag.jpg  \n",
            " extracting: test/images/000011_jpg.rf.2rOVSGG83QcTZ9Mccgtu.jpg  \n",
            " extracting: test/images/000034_jpg.rf.6tAgo1bPQTdMYVV11pUv.jpg  \n",
            " extracting: test/images/000047_jpg.rf.FoHEHNTXr1caqFBx13Fy.jpg  \n",
            " extracting: test/images/000054_jpg.rf.T27M0tW8SI0Z9B8fLRDU.jpg  \n",
            " extracting: test/images/000073_jpg.rf.iYoZkrbuFpmIoTtUOkfB.jpg  \n",
            " extracting: test/images/000076_jpg.rf.UrNpgKEsa9fWs8uhsBHd.jpg  \n",
            " extracting: test/images/000084_jpg.rf.mmzwAuMnrHRDIXvDANas.jpg  \n",
            " extracting: test/images/000097_jpg.rf.JPLKrtlTviASI4uvfM4g.jpg  \n",
            " extracting: test/images/000098_jpg.rf.pkibipXn8qKOL8w7fdGK.jpg  \n",
            "   creating: test/labels/\n",
            " extracting: test/labels/000008_jpg.rf.mR8kVxlPQ0Cc2xInZzag.txt  \n",
            " extracting: test/labels/000011_jpg.rf.2rOVSGG83QcTZ9Mccgtu.txt  \n",
            " extracting: test/labels/000034_jpg.rf.6tAgo1bPQTdMYVV11pUv.txt  \n",
            " extracting: test/labels/000047_jpg.rf.FoHEHNTXr1caqFBx13Fy.txt  \n",
            " extracting: test/labels/000054_jpg.rf.T27M0tW8SI0Z9B8fLRDU.txt  \n",
            " extracting: test/labels/000073_jpg.rf.iYoZkrbuFpmIoTtUOkfB.txt  \n",
            " extracting: test/labels/000076_jpg.rf.UrNpgKEsa9fWs8uhsBHd.txt  \n",
            " extracting: test/labels/000084_jpg.rf.mmzwAuMnrHRDIXvDANas.txt  \n",
            " extracting: test/labels/000097_jpg.rf.JPLKrtlTviASI4uvfM4g.txt  \n",
            " extracting: test/labels/000098_jpg.rf.pkibipXn8qKOL8w7fdGK.txt  \n",
            "   creating: train/\n",
            "   creating: train/images/\n",
            " extracting: train/images/000001_jpg.rf.7U0O47bMa1wfNyY3i05E.jpg  \n",
            " extracting: train/images/000001_jpg.rf.WdT8x2eNZfGknE3Zu9Io.jpg  \n",
            " extracting: train/images/000001_jpg.rf.n4GuHHWP6Iih13lVjy3V.jpg  \n",
            " extracting: train/images/000007_jpg.rf.9zMqdt4MUCgh6gnHYEQ6.jpg  \n",
            " extracting: train/images/000007_jpg.rf.Umz80YV32PpbMU0N7yDc.jpg  \n",
            " extracting: train/images/000007_jpg.rf.w1KOrpDXv1dFXd5hTwmj.jpg  \n",
            " extracting: train/images/000012_jpg.rf.eqYRChY0UvBTmZ9t26NQ.jpg  \n",
            " extracting: train/images/000012_jpg.rf.gUBEPsefz95Vlul58fE3.jpg  \n",
            " extracting: train/images/000012_jpg.rf.ljwLekewKXavMvO5YbMi.jpg  \n",
            " extracting: train/images/000013_jpg.rf.3SfGtkslcmQhaQ6vADUl.jpg  \n",
            " extracting: train/images/000013_jpg.rf.iX0yl51EaKqV7ZXaSAX5.jpg  \n",
            " extracting: train/images/000013_jpg.rf.oWA39d64Dr5Lrz98FraW.jpg  \n",
            " extracting: train/images/000014_jpg.rf.EId1mU6vAmIrd1iyj0VU.jpg  \n",
            " extracting: train/images/000014_jpg.rf.kVcgSzJg6EEjBqWMpSJV.jpg  \n",
            " extracting: train/images/000014_jpg.rf.zW1dr4rMysjocPhhAf2w.jpg  \n",
            " extracting: train/images/000016_jpg.rf.IMgYHiLkdK8TIwcBbeCD.jpg  \n",
            " extracting: train/images/000016_jpg.rf.oS15wLGQzo74AVAShe7G.jpg  \n",
            " extracting: train/images/000016_jpg.rf.yrXaQobpfhmiRR7mxVsi.jpg  \n",
            " extracting: train/images/000017_jpg.rf.HQ41tmSKoKQAUVXm5up2.jpg  \n",
            " extracting: train/images/000017_jpg.rf.J0amz5LpabQKf9Yijn6J.jpg  \n",
            " extracting: train/images/000017_jpg.rf.TkdQa9WyQ5AxnVB6Yrpp.jpg  \n",
            " extracting: train/images/000018_jpg.rf.GwHYigMMPhQ6bRdZWU3Y.jpg  \n",
            " extracting: train/images/000018_jpg.rf.u3n1M6cHQQ5aX6pFpPxa.jpg  \n",
            " extracting: train/images/000018_jpg.rf.ulSiYh3JMH8oJBSs9uG7.jpg  \n",
            " extracting: train/images/000019_jpg.rf.f5vGf7Xh8Snzdl5sQhWU.jpg  \n",
            " extracting: train/images/000019_jpg.rf.f9HpZ2qTb0wWM2yj9gmW.jpg  \n",
            " extracting: train/images/000019_jpg.rf.tbkn0soS9obcyhc85yZf.jpg  \n",
            " extracting: train/images/000020_jpg.rf.OVtUL8J0DAkkv8WjJOLq.jpg  \n",
            " extracting: train/images/000020_jpg.rf.RcyH2e0gcdrp0iT87giw.jpg  \n",
            " extracting: train/images/000020_jpg.rf.rBnGQPyuq6tKm75HR1by.jpg  \n",
            " extracting: train/images/000021_jpg.rf.TCqdBtW05mly0fpgIMJD.jpg  \n",
            " extracting: train/images/000021_jpg.rf.h6XZXk2blgN6VywkDTVV.jpg  \n",
            " extracting: train/images/000021_jpg.rf.ypLjKJ7hJ8UYHlg25a3V.jpg  \n",
            " extracting: train/images/000022_jpg.rf.59NILNxTRaLHHVB15uRo.jpg  \n",
            " extracting: train/images/000022_jpg.rf.S4tHUZFRQ1Y80gwRC4C2.jpg  \n",
            " extracting: train/images/000022_jpg.rf.uHuTfqGIcHD3lEW8P3n2.jpg  \n",
            " extracting: train/images/000023_jpg.rf.5HueWOnscQm9yP2SdZoL.jpg  \n",
            " extracting: train/images/000023_jpg.rf.ZysWiKsjKmYVkRUnZL2b.jpg  \n",
            " extracting: train/images/000023_jpg.rf.q71NllU7feZhOPdg5fZO.jpg  \n",
            " extracting: train/images/000024_jpg.rf.BgHdz3ClnbBmKZ03HoaB.jpg  \n",
            " extracting: train/images/000024_jpg.rf.lQs1YCmIw78rbiFUvaXm.jpg  \n",
            " extracting: train/images/000024_jpg.rf.yNKGMKeGA7OGHqlaX3Rh.jpg  \n",
            " extracting: train/images/000025_jpg.rf.n4gp8jx641drjwph99LU.jpg  \n",
            " extracting: train/images/000025_jpg.rf.vBv1gJZR1MfrhzbQuLG2.jpg  \n",
            " extracting: train/images/000025_jpg.rf.z3CCzJrrQ0H6SGInXqsa.jpg  \n",
            " extracting: train/images/000026_jpg.rf.2A2gkT0pdvHW7FC98YlH.jpg  \n",
            " extracting: train/images/000026_jpg.rf.DZQBPrDizqBrBPBudtdO.jpg  \n",
            " extracting: train/images/000026_jpg.rf.utJccJLxAYBpCVKCRebO.jpg  \n",
            " extracting: train/images/000027_jpg.rf.WEj8PxYjFHM2xBt1sGlz.jpg  \n",
            " extracting: train/images/000027_jpg.rf.pJtmtbVlcmjx2VEGieyr.jpg  \n",
            " extracting: train/images/000027_jpg.rf.twHHC5QTw0IIWNXjXXVJ.jpg  \n",
            " extracting: train/images/000028_jpg.rf.4ScEcshM5gYnvzbodZi0.jpg  \n",
            " extracting: train/images/000028_jpg.rf.RE7jyJwt0rgCgJZDWqPs.jpg  \n",
            " extracting: train/images/000028_jpg.rf.SD7x1VkvpF0IUJYAgcIv.jpg  \n",
            " extracting: train/images/000029_jpg.rf.HRBezj4pg3UUOiwRvnnF.jpg  \n",
            " extracting: train/images/000029_jpg.rf.MQGJ6IbOkB1i0LRJJ4lO.jpg  \n",
            " extracting: train/images/000029_jpg.rf.oj5HQlPkS9xxEbbrqAr9.jpg  \n",
            " extracting: train/images/000030_jpg.rf.IYWUWTY7dt8wxWjliQHX.jpg  \n",
            " extracting: train/images/000030_jpg.rf.RUJIOSJAIWW4ogPfeIw6.jpg  \n",
            " extracting: train/images/000030_jpg.rf.gJSkZGb9XgdgPxiCzOw0.jpg  \n",
            " extracting: train/images/000031_jpg.rf.Q1CKMjUuLq8pnu9jP9Nx.jpg  \n",
            " extracting: train/images/000031_jpg.rf.qQOhkt95hFtottzBTrzh.jpg  \n",
            " extracting: train/images/000031_jpg.rf.xREnRHdmATmDQCvkROCj.jpg  \n",
            " extracting: train/images/000032_jpg.rf.SvyO7W17xrv7kZg4Zv8v.jpg  \n",
            " extracting: train/images/000032_jpg.rf.cup0jjbAUw0N0AwRIe9Q.jpg  \n",
            " extracting: train/images/000032_jpg.rf.uZkHijWQwis7yLchKSie.jpg  \n",
            " extracting: train/images/000033_jpg.rf.7c2jfyipYonkpRjd7Swl.jpg  \n",
            " extracting: train/images/000033_jpg.rf.8kedKVyaCQovk0n7qh31.jpg  \n",
            " extracting: train/images/000033_jpg.rf.wNLfeFOoO86kQVYqFtHk.jpg  \n",
            " extracting: train/images/000035_jpg.rf.LAWLSCjmO65TnFpGhBh9.jpg  \n",
            " extracting: train/images/000035_jpg.rf.TxPfrBG0f0DS6CP9a74T.jpg  \n",
            " extracting: train/images/000035_jpg.rf.xZwzUtwJlc2ORR54cIun.jpg  \n",
            " extracting: train/images/000036_jpg.rf.U7Qw4YwZ1CpNH8ohXl4M.jpg  \n",
            " extracting: train/images/000036_jpg.rf.b9tEA9AOKC2VW4QMslkF.jpg  \n",
            " extracting: train/images/000036_jpg.rf.j6Lvj47GBRkX8nOUPPqY.jpg  \n",
            " extracting: train/images/000037_jpg.rf.jh3FDtATkYAZwTCWJTQq.jpg  \n",
            " extracting: train/images/000037_jpg.rf.k3CcxR4zOlhbkwrjhjBn.jpg  \n",
            " extracting: train/images/000037_jpg.rf.xcg3EXqlncPrcKKbpplg.jpg  \n",
            " extracting: train/images/000038_jpg.rf.Y8fJkSqKHGezakzILFam.jpg  \n",
            " extracting: train/images/000038_jpg.rf.fn0DAr40hsKIP1Blhh5J.jpg  \n",
            " extracting: train/images/000038_jpg.rf.z7ceQqIjr1iTR2Vrg73L.jpg  \n",
            " extracting: train/images/000039_jpg.rf.GShp9Ixe3GjjVGd3JoY6.jpg  \n",
            " extracting: train/images/000039_jpg.rf.SfiboFuAwUNBBboftdA6.jpg  \n",
            " extracting: train/images/000039_jpg.rf.T8r6GwINvZupzIpB43oS.jpg  \n",
            " extracting: train/images/000040_jpg.rf.8x7HuadCzeDX1wYVJ4Fz.jpg  \n",
            " extracting: train/images/000040_jpg.rf.C1fkemY90MlahfFcraly.jpg  \n",
            " extracting: train/images/000040_jpg.rf.kps4Ac8ckN74ERUzgBuW.jpg  \n",
            " extracting: train/images/000041_jpg.rf.06IlN74ai9AlqtafQM4F.jpg  \n",
            " extracting: train/images/000041_jpg.rf.XOyNAb6tFBBdzqKWtO9Y.jpg  \n",
            " extracting: train/images/000041_jpg.rf.wxyuDLbDif9CffLG4XcS.jpg  \n",
            " extracting: train/images/000042_jpg.rf.LX1Ldbpg8B8N1iFPYX2M.jpg  \n",
            " extracting: train/images/000042_jpg.rf.boNJxnU9hJgSneRhFP7D.jpg  \n",
            " extracting: train/images/000042_jpg.rf.mqXvkCTGfUEe2xgsSICz.jpg  \n",
            " extracting: train/images/000044_jpg.rf.SYRauAcniRu5jvL6Ron8.jpg  \n",
            " extracting: train/images/000044_jpg.rf.ZDEWDkQ3M6qbIIaGLx3f.jpg  \n",
            " extracting: train/images/000044_jpg.rf.o2BMmROzRuZ9aT21GxaL.jpg  \n",
            " extracting: train/images/000045_jpg.rf.LD196FgLojNFA3EsW1yH.jpg  \n",
            " extracting: train/images/000045_jpg.rf.Q6b66OQWhAHcL96vQ7EE.jpg  \n",
            " extracting: train/images/000045_jpg.rf.ZizjyKFwVcEpRPykVgGm.jpg  \n",
            " extracting: train/images/000049_jpg.rf.6lTDiDnwTse3Iq4gQzzV.jpg  \n",
            " extracting: train/images/000049_jpg.rf.GgL6T6SaIRFnvkGdrxBO.jpg  \n",
            " extracting: train/images/000049_jpg.rf.mYV5i7eycJQF3T0BfxAn.jpg  \n",
            " extracting: train/images/000050_jpg.rf.05kvW6nlOzpKGVamskXA.jpg  \n",
            " extracting: train/images/000050_jpg.rf.D6WmrkJWJ3sCPooFTSjI.jpg  \n",
            " extracting: train/images/000050_jpg.rf.KTvC7f7jlGrSUJjYuKzs.jpg  \n",
            " extracting: train/images/000051_jpg.rf.QRsxhp7D7XSuvloHt8pT.jpg  \n",
            " extracting: train/images/000051_jpg.rf.UoyBuif8pU0x4YhFmbzx.jpg  \n",
            " extracting: train/images/000051_jpg.rf.uDVYrfPifvhsTznHvjK7.jpg  \n",
            " extracting: train/images/000052_jpg.rf.2ZnP36WFK4RIMGpJc01W.jpg  \n",
            " extracting: train/images/000052_jpg.rf.4dlTo7uHxfJ4Xt7hy7SO.jpg  \n",
            " extracting: train/images/000052_jpg.rf.LoA9igghDdAMwRTpflkK.jpg  \n",
            " extracting: train/images/000053_jpg.rf.CKbDRke2tRgXEsPhKzZJ.jpg  \n",
            " extracting: train/images/000053_jpg.rf.Co4eoGvBcpzq6J2zio2J.jpg  \n",
            " extracting: train/images/000053_jpg.rf.Vyoh6m3RFZ341XOzkuRb.jpg  \n",
            " extracting: train/images/000056_jpg.rf.0a4gr3SSIGzaWJGBa9CV.jpg  \n",
            " extracting: train/images/000056_jpg.rf.2CelKwCEU3z2K5CrcA6L.jpg  \n",
            " extracting: train/images/000056_jpg.rf.SL4Cfhzt7BshzL80dEvl.jpg  \n",
            " extracting: train/images/000058_jpg.rf.B9XHIcuwIYB89C2bjuc4.jpg  \n",
            " extracting: train/images/000058_jpg.rf.diJde8x4u6pbsFMSHzEZ.jpg  \n",
            " extracting: train/images/000058_jpg.rf.iPaT4SUlgXx5dh8R4jTG.jpg  \n",
            " extracting: train/images/000059_jpg.rf.43tEU54pUwvLHman2f2L.jpg  \n",
            " extracting: train/images/000059_jpg.rf.h4YNG9uB5YjBAZtJzCFJ.jpg  \n",
            " extracting: train/images/000059_jpg.rf.nHQY1dTBsNrCLwKgxMiR.jpg  \n",
            " extracting: train/images/000060_jpg.rf.Ezu21nMPvhWarOAhzhVN.jpg  \n",
            " extracting: train/images/000060_jpg.rf.Fo86oGpZklm537JRijIJ.jpg  \n",
            " extracting: train/images/000060_jpg.rf.Ph3mdJbBSVVUMV96KV2A.jpg  \n",
            " extracting: train/images/000061_jpg.rf.9rceQGgVUWOnoFRLEXe8.jpg  \n",
            " extracting: train/images/000061_jpg.rf.hn5AjDoonGQxmE1lnZIy.jpg  \n",
            " extracting: train/images/000061_jpg.rf.uNGIgLhOEV0cydH3ZwR6.jpg  \n",
            " extracting: train/images/000062_jpg.rf.XwAowGfpKTUWSJmVFaiA.jpg  \n",
            " extracting: train/images/000062_jpg.rf.fKhBTYmddvoY0FmhJKDP.jpg  \n",
            " extracting: train/images/000062_jpg.rf.uwLNGlHZySbja0WiKLiV.jpg  \n",
            " extracting: train/images/000064_jpg.rf.PlcuzIdUDG9YD2WjUSb8.jpg  \n",
            " extracting: train/images/000064_jpg.rf.StmrhCuB7nehk4zEDpky.jpg  \n",
            " extracting: train/images/000064_jpg.rf.ZztH6MoKg9va3fhkwvpT.jpg  \n",
            " extracting: train/images/000066_jpg.rf.4OG1QXxFH3N8qCudadHd.jpg  \n",
            " extracting: train/images/000066_jpg.rf.p8K3hXAOf74M6Zwc4hpX.jpg  \n",
            " extracting: train/images/000066_jpg.rf.xNp07mcDxrw2PKGm5ApN.jpg  \n",
            " extracting: train/images/000067_jpg.rf.AU9S8lcvCxzkNncyJEcW.jpg  \n",
            " extracting: train/images/000067_jpg.rf.phDbe78mDYxNjfxhw12u.jpg  \n",
            " extracting: train/images/000067_jpg.rf.x8bC5BrhxJAbivtlaoio.jpg  \n",
            " extracting: train/images/000068_jpg.rf.2a8QNKFhsevAVDGXayKX.jpg  \n",
            " extracting: train/images/000068_jpg.rf.gfP54GQ6ESCxzdhBYI1a.jpg  \n",
            " extracting: train/images/000068_jpg.rf.kt7IgkP9T9TCS38SOytV.jpg  \n",
            " extracting: train/images/000069_jpg.rf.Pc7hiToi0T6cRsmJH0OW.jpg  \n",
            " extracting: train/images/000069_jpg.rf.frJxgBn5VxdulzAyVivx.jpg  \n",
            " extracting: train/images/000069_jpg.rf.x3ceAocjuwAIANvuTaRy.jpg  \n",
            " extracting: train/images/000070_jpg.rf.8qdHFoKKBIAyrUQ3rpoU.jpg  \n",
            " extracting: train/images/000070_jpg.rf.DYa8vJbo4YaynXD9GmL2.jpg  \n",
            " extracting: train/images/000070_jpg.rf.JoHfNbdfZ8OdLn5ditWv.jpg  \n",
            " extracting: train/images/000072_jpg.rf.P4lshP7NUFWOBoPV5Tf9.jpg  \n",
            " extracting: train/images/000072_jpg.rf.jkyDVIc64m6QAlMW3hoD.jpg  \n",
            " extracting: train/images/000072_jpg.rf.qdicqt6kCH9rGNVEOpPh.jpg  \n",
            " extracting: train/images/000074_jpg.rf.2m3knj6pDI8IRWmxhGnE.jpg  \n",
            " extracting: train/images/000074_jpg.rf.CYrm6uUkUH8AGkObfRf7.jpg  \n",
            " extracting: train/images/000074_jpg.rf.WWphez97zt1EDabxVtZj.jpg  \n",
            " extracting: train/images/000075_jpg.rf.Kb5jECyf1XjFK60KnXDW.jpg  \n",
            " extracting: train/images/000075_jpg.rf.T1kKTwyKp5sPcoLklunT.jpg  \n",
            " extracting: train/images/000075_jpg.rf.uBDuTQ0sb5cO0CDjYwuP.jpg  \n",
            " extracting: train/images/000077_jpg.rf.QgiM7UiYM06PMHua94Bz.jpg  \n",
            " extracting: train/images/000077_jpg.rf.f2hRQ4wEoKAZ3DMqWjJC.jpg  \n",
            " extracting: train/images/000077_jpg.rf.kum6CWTGwuntrBG6jM4H.jpg  \n",
            " extracting: train/images/000078_jpg.rf.Am2M4dYnUUehPP5d2ng7.jpg  \n",
            " extracting: train/images/000078_jpg.rf.aJ4glOM4IuISluylbu4d.jpg  \n",
            " extracting: train/images/000078_jpg.rf.txjrIWpIWnYlG2yZU8oP.jpg  \n",
            " extracting: train/images/000079_jpg.rf.Kl8bcYNYSVIhhYfq1rCI.jpg  \n",
            " extracting: train/images/000079_jpg.rf.hY5tpB8hEjafbYmlPNNU.jpg  \n",
            " extracting: train/images/000079_jpg.rf.m6Af8XgalPfGHM9uWaHO.jpg  \n",
            " extracting: train/images/000080_jpg.rf.XJujCarCyuraqyGSoMgh.jpg  \n",
            " extracting: train/images/000080_jpg.rf.ZfXapbI53t3jmPkKIdaD.jpg  \n",
            " extracting: train/images/000080_jpg.rf.ddBfIzWIV6vhJt9EYtKG.jpg  \n",
            " extracting: train/images/000081_jpg.rf.auiCT6gKIL9VJWpesW4H.jpg  \n",
            " extracting: train/images/000081_jpg.rf.lXW06Ef5UDaODlUJI7hp.jpg  \n",
            " extracting: train/images/000081_jpg.rf.pXr6hIqs8MKLe2Sgjw0s.jpg  \n",
            " extracting: train/images/000082_jpg.rf.JApjBNvaNk6vp7rznKZz.jpg  \n",
            " extracting: train/images/000082_jpg.rf.YXz0obtreRGSz5Wx3Ey1.jpg  \n",
            " extracting: train/images/000082_jpg.rf.u4sgJAx8DpUVB63Y2RoK.jpg  \n",
            " extracting: train/images/000083_jpg.rf.i90UF3uqLUo0x1UUkDuh.jpg  \n",
            " extracting: train/images/000083_jpg.rf.kUwbglpTLRNBrTF4u9EQ.jpg  \n",
            " extracting: train/images/000083_jpg.rf.rHeuCMgPUHCzCG756dSW.jpg  \n",
            " extracting: train/images/000085_jpg.rf.HRqvRkJXUQxr85CNq4Yv.jpg  \n",
            " extracting: train/images/000085_jpg.rf.ojZTHjtQkhIdQZGtLAGI.jpg  \n",
            " extracting: train/images/000085_jpg.rf.p1vKiX1XZVeNzxz21xO0.jpg  \n",
            " extracting: train/images/000086_jpg.rf.CxPgEdd3mbV6evmaCp49.jpg  \n",
            " extracting: train/images/000086_jpg.rf.OpueTNVC1FnN1rGHc5v0.jpg  \n",
            " extracting: train/images/000086_jpg.rf.X1s7KaJvG0v6gkevWWWU.jpg  \n",
            " extracting: train/images/000087_jpg.rf.EgZkP3DUfudWAVGTl4lh.jpg  \n",
            " extracting: train/images/000087_jpg.rf.hH4SrRIge12i9f9R4U4J.jpg  \n",
            " extracting: train/images/000087_jpg.rf.nbDoL6ci6E9ACVmXcZHg.jpg  \n",
            " extracting: train/images/000089_jpg.rf.Ba6OX2YxVIZIZ1NQ7BPC.jpg  \n",
            " extracting: train/images/000089_jpg.rf.RZSak1vtx2CmU4i9bjYA.jpg  \n",
            " extracting: train/images/000089_jpg.rf.i8RDm0isVu7bkLWR6UMR.jpg  \n",
            " extracting: train/images/000090_jpg.rf.TydapqS4Mmh9dto5obLJ.jpg  \n",
            " extracting: train/images/000090_jpg.rf.UA9rk7GIjZYyTBHKkxOZ.jpg  \n",
            " extracting: train/images/000090_jpg.rf.VVyrrhRCtnezzTqzC9Cb.jpg  \n",
            " extracting: train/images/000092_jpg.rf.34crkSGa86TCRm3RE9nd.jpg  \n",
            " extracting: train/images/000092_jpg.rf.AtiTFVFe7pc7DD4oulad.jpg  \n",
            " extracting: train/images/000092_jpg.rf.W5NQzGcFgb0bCheLCuqv.jpg  \n",
            " extracting: train/images/000093_jpg.rf.MDerlMiMf0r5JC6rkGxS.jpg  \n",
            " extracting: train/images/000093_jpg.rf.VXRxCQKS5nYyA8Oh21cj.jpg  \n",
            " extracting: train/images/000093_jpg.rf.Vt4A9IytAtIJV3lXHzEV.jpg  \n",
            " extracting: train/images/000094_jpg.rf.EmnfIwqIRrEO4dCfjoty.jpg  \n",
            " extracting: train/images/000094_jpg.rf.XNS73rJJcgzh4GjxdBti.jpg  \n",
            " extracting: train/images/000094_jpg.rf.jeuuncHqmcnlnS1GODg6.jpg  \n",
            " extracting: train/images/000096_jpg.rf.5wfNybuHvYMOjE56dIGW.jpg  \n",
            " extracting: train/images/000096_jpg.rf.FDtlsfqlNYS8290694xq.jpg  \n",
            " extracting: train/images/000096_jpg.rf.ixWWVlafcwTHcJ1gJtKc.jpg  \n",
            " extracting: train/images/000100_jpg.rf.FNzjBw2Sw5uu59aao3Ah.jpg  \n",
            " extracting: train/images/000100_jpg.rf.XXrB6X48vPfpWlqSt6fp.jpg  \n",
            " extracting: train/images/000100_jpg.rf.pmGwo5ADKklyj1dvkPM2.jpg  \n",
            "   creating: train/labels/\n",
            " extracting: train/labels/000001_jpg.rf.7U0O47bMa1wfNyY3i05E.txt  \n",
            " extracting: train/labels/000001_jpg.rf.WdT8x2eNZfGknE3Zu9Io.txt  \n",
            " extracting: train/labels/000001_jpg.rf.n4GuHHWP6Iih13lVjy3V.txt  \n",
            " extracting: train/labels/000007_jpg.rf.9zMqdt4MUCgh6gnHYEQ6.txt  \n",
            " extracting: train/labels/000007_jpg.rf.Umz80YV32PpbMU0N7yDc.txt  \n",
            " extracting: train/labels/000007_jpg.rf.w1KOrpDXv1dFXd5hTwmj.txt  \n",
            " extracting: train/labels/000012_jpg.rf.eqYRChY0UvBTmZ9t26NQ.txt  \n",
            " extracting: train/labels/000012_jpg.rf.gUBEPsefz95Vlul58fE3.txt  \n",
            " extracting: train/labels/000012_jpg.rf.ljwLekewKXavMvO5YbMi.txt  \n",
            " extracting: train/labels/000013_jpg.rf.3SfGtkslcmQhaQ6vADUl.txt  \n",
            " extracting: train/labels/000013_jpg.rf.iX0yl51EaKqV7ZXaSAX5.txt  \n",
            " extracting: train/labels/000013_jpg.rf.oWA39d64Dr5Lrz98FraW.txt  \n",
            " extracting: train/labels/000014_jpg.rf.EId1mU6vAmIrd1iyj0VU.txt  \n",
            " extracting: train/labels/000014_jpg.rf.kVcgSzJg6EEjBqWMpSJV.txt  \n",
            " extracting: train/labels/000014_jpg.rf.zW1dr4rMysjocPhhAf2w.txt  \n",
            " extracting: train/labels/000016_jpg.rf.IMgYHiLkdK8TIwcBbeCD.txt  \n",
            " extracting: train/labels/000016_jpg.rf.oS15wLGQzo74AVAShe7G.txt  \n",
            " extracting: train/labels/000016_jpg.rf.yrXaQobpfhmiRR7mxVsi.txt  \n",
            " extracting: train/labels/000017_jpg.rf.HQ41tmSKoKQAUVXm5up2.txt  \n",
            " extracting: train/labels/000017_jpg.rf.J0amz5LpabQKf9Yijn6J.txt  \n",
            " extracting: train/labels/000017_jpg.rf.TkdQa9WyQ5AxnVB6Yrpp.txt  \n",
            " extracting: train/labels/000018_jpg.rf.GwHYigMMPhQ6bRdZWU3Y.txt  \n",
            " extracting: train/labels/000018_jpg.rf.u3n1M6cHQQ5aX6pFpPxa.txt  \n",
            " extracting: train/labels/000018_jpg.rf.ulSiYh3JMH8oJBSs9uG7.txt  \n",
            " extracting: train/labels/000019_jpg.rf.f5vGf7Xh8Snzdl5sQhWU.txt  \n",
            " extracting: train/labels/000019_jpg.rf.f9HpZ2qTb0wWM2yj9gmW.txt  \n",
            " extracting: train/labels/000019_jpg.rf.tbkn0soS9obcyhc85yZf.txt  \n",
            " extracting: train/labels/000020_jpg.rf.OVtUL8J0DAkkv8WjJOLq.txt  \n",
            " extracting: train/labels/000020_jpg.rf.RcyH2e0gcdrp0iT87giw.txt  \n",
            " extracting: train/labels/000020_jpg.rf.rBnGQPyuq6tKm75HR1by.txt  \n",
            " extracting: train/labels/000021_jpg.rf.TCqdBtW05mly0fpgIMJD.txt  \n",
            " extracting: train/labels/000021_jpg.rf.h6XZXk2blgN6VywkDTVV.txt  \n",
            " extracting: train/labels/000021_jpg.rf.ypLjKJ7hJ8UYHlg25a3V.txt  \n",
            " extracting: train/labels/000022_jpg.rf.59NILNxTRaLHHVB15uRo.txt  \n",
            " extracting: train/labels/000022_jpg.rf.S4tHUZFRQ1Y80gwRC4C2.txt  \n",
            " extracting: train/labels/000022_jpg.rf.uHuTfqGIcHD3lEW8P3n2.txt  \n",
            " extracting: train/labels/000023_jpg.rf.5HueWOnscQm9yP2SdZoL.txt  \n",
            " extracting: train/labels/000023_jpg.rf.ZysWiKsjKmYVkRUnZL2b.txt  \n",
            " extracting: train/labels/000023_jpg.rf.q71NllU7feZhOPdg5fZO.txt  \n",
            " extracting: train/labels/000024_jpg.rf.BgHdz3ClnbBmKZ03HoaB.txt  \n",
            " extracting: train/labels/000024_jpg.rf.lQs1YCmIw78rbiFUvaXm.txt  \n",
            " extracting: train/labels/000024_jpg.rf.yNKGMKeGA7OGHqlaX3Rh.txt  \n",
            " extracting: train/labels/000025_jpg.rf.n4gp8jx641drjwph99LU.txt  \n",
            " extracting: train/labels/000025_jpg.rf.vBv1gJZR1MfrhzbQuLG2.txt  \n",
            " extracting: train/labels/000025_jpg.rf.z3CCzJrrQ0H6SGInXqsa.txt  \n",
            " extracting: train/labels/000026_jpg.rf.2A2gkT0pdvHW7FC98YlH.txt  \n",
            " extracting: train/labels/000026_jpg.rf.DZQBPrDizqBrBPBudtdO.txt  \n",
            " extracting: train/labels/000026_jpg.rf.utJccJLxAYBpCVKCRebO.txt  \n",
            " extracting: train/labels/000027_jpg.rf.WEj8PxYjFHM2xBt1sGlz.txt  \n",
            " extracting: train/labels/000027_jpg.rf.pJtmtbVlcmjx2VEGieyr.txt  \n",
            " extracting: train/labels/000027_jpg.rf.twHHC5QTw0IIWNXjXXVJ.txt  \n",
            " extracting: train/labels/000028_jpg.rf.4ScEcshM5gYnvzbodZi0.txt  \n",
            " extracting: train/labels/000028_jpg.rf.RE7jyJwt0rgCgJZDWqPs.txt  \n",
            " extracting: train/labels/000028_jpg.rf.SD7x1VkvpF0IUJYAgcIv.txt  \n",
            " extracting: train/labels/000029_jpg.rf.HRBezj4pg3UUOiwRvnnF.txt  \n",
            " extracting: train/labels/000029_jpg.rf.MQGJ6IbOkB1i0LRJJ4lO.txt  \n",
            " extracting: train/labels/000029_jpg.rf.oj5HQlPkS9xxEbbrqAr9.txt  \n",
            " extracting: train/labels/000030_jpg.rf.IYWUWTY7dt8wxWjliQHX.txt  \n",
            " extracting: train/labels/000030_jpg.rf.RUJIOSJAIWW4ogPfeIw6.txt  \n",
            " extracting: train/labels/000030_jpg.rf.gJSkZGb9XgdgPxiCzOw0.txt  \n",
            " extracting: train/labels/000031_jpg.rf.Q1CKMjUuLq8pnu9jP9Nx.txt  \n",
            " extracting: train/labels/000031_jpg.rf.qQOhkt95hFtottzBTrzh.txt  \n",
            " extracting: train/labels/000031_jpg.rf.xREnRHdmATmDQCvkROCj.txt  \n",
            " extracting: train/labels/000032_jpg.rf.SvyO7W17xrv7kZg4Zv8v.txt  \n",
            " extracting: train/labels/000032_jpg.rf.cup0jjbAUw0N0AwRIe9Q.txt  \n",
            " extracting: train/labels/000032_jpg.rf.uZkHijWQwis7yLchKSie.txt  \n",
            " extracting: train/labels/000033_jpg.rf.7c2jfyipYonkpRjd7Swl.txt  \n",
            " extracting: train/labels/000033_jpg.rf.8kedKVyaCQovk0n7qh31.txt  \n",
            " extracting: train/labels/000033_jpg.rf.wNLfeFOoO86kQVYqFtHk.txt  \n",
            " extracting: train/labels/000035_jpg.rf.LAWLSCjmO65TnFpGhBh9.txt  \n",
            " extracting: train/labels/000035_jpg.rf.TxPfrBG0f0DS6CP9a74T.txt  \n",
            " extracting: train/labels/000035_jpg.rf.xZwzUtwJlc2ORR54cIun.txt  \n",
            " extracting: train/labels/000036_jpg.rf.U7Qw4YwZ1CpNH8ohXl4M.txt  \n",
            " extracting: train/labels/000036_jpg.rf.b9tEA9AOKC2VW4QMslkF.txt  \n",
            " extracting: train/labels/000036_jpg.rf.j6Lvj47GBRkX8nOUPPqY.txt  \n",
            " extracting: train/labels/000037_jpg.rf.jh3FDtATkYAZwTCWJTQq.txt  \n",
            " extracting: train/labels/000037_jpg.rf.k3CcxR4zOlhbkwrjhjBn.txt  \n",
            " extracting: train/labels/000037_jpg.rf.xcg3EXqlncPrcKKbpplg.txt  \n",
            " extracting: train/labels/000038_jpg.rf.Y8fJkSqKHGezakzILFam.txt  \n",
            " extracting: train/labels/000038_jpg.rf.fn0DAr40hsKIP1Blhh5J.txt  \n",
            " extracting: train/labels/000038_jpg.rf.z7ceQqIjr1iTR2Vrg73L.txt  \n",
            " extracting: train/labels/000039_jpg.rf.GShp9Ixe3GjjVGd3JoY6.txt  \n",
            " extracting: train/labels/000039_jpg.rf.SfiboFuAwUNBBboftdA6.txt  \n",
            " extracting: train/labels/000039_jpg.rf.T8r6GwINvZupzIpB43oS.txt  \n",
            " extracting: train/labels/000040_jpg.rf.8x7HuadCzeDX1wYVJ4Fz.txt  \n",
            " extracting: train/labels/000040_jpg.rf.C1fkemY90MlahfFcraly.txt  \n",
            " extracting: train/labels/000040_jpg.rf.kps4Ac8ckN74ERUzgBuW.txt  \n",
            " extracting: train/labels/000041_jpg.rf.06IlN74ai9AlqtafQM4F.txt  \n",
            " extracting: train/labels/000041_jpg.rf.XOyNAb6tFBBdzqKWtO9Y.txt  \n",
            " extracting: train/labels/000041_jpg.rf.wxyuDLbDif9CffLG4XcS.txt  \n",
            " extracting: train/labels/000042_jpg.rf.LX1Ldbpg8B8N1iFPYX2M.txt  \n",
            " extracting: train/labels/000042_jpg.rf.boNJxnU9hJgSneRhFP7D.txt  \n",
            " extracting: train/labels/000042_jpg.rf.mqXvkCTGfUEe2xgsSICz.txt  \n",
            " extracting: train/labels/000044_jpg.rf.SYRauAcniRu5jvL6Ron8.txt  \n",
            " extracting: train/labels/000044_jpg.rf.ZDEWDkQ3M6qbIIaGLx3f.txt  \n",
            " extracting: train/labels/000044_jpg.rf.o2BMmROzRuZ9aT21GxaL.txt  \n",
            " extracting: train/labels/000045_jpg.rf.LD196FgLojNFA3EsW1yH.txt  \n",
            " extracting: train/labels/000045_jpg.rf.Q6b66OQWhAHcL96vQ7EE.txt  \n",
            " extracting: train/labels/000045_jpg.rf.ZizjyKFwVcEpRPykVgGm.txt  \n",
            " extracting: train/labels/000049_jpg.rf.6lTDiDnwTse3Iq4gQzzV.txt  \n",
            " extracting: train/labels/000049_jpg.rf.GgL6T6SaIRFnvkGdrxBO.txt  \n",
            " extracting: train/labels/000049_jpg.rf.mYV5i7eycJQF3T0BfxAn.txt  \n",
            " extracting: train/labels/000050_jpg.rf.05kvW6nlOzpKGVamskXA.txt  \n",
            " extracting: train/labels/000050_jpg.rf.D6WmrkJWJ3sCPooFTSjI.txt  \n",
            " extracting: train/labels/000050_jpg.rf.KTvC7f7jlGrSUJjYuKzs.txt  \n",
            " extracting: train/labels/000051_jpg.rf.QRsxhp7D7XSuvloHt8pT.txt  \n",
            " extracting: train/labels/000051_jpg.rf.UoyBuif8pU0x4YhFmbzx.txt  \n",
            " extracting: train/labels/000051_jpg.rf.uDVYrfPifvhsTznHvjK7.txt  \n",
            " extracting: train/labels/000052_jpg.rf.2ZnP36WFK4RIMGpJc01W.txt  \n",
            " extracting: train/labels/000052_jpg.rf.4dlTo7uHxfJ4Xt7hy7SO.txt  \n",
            " extracting: train/labels/000052_jpg.rf.LoA9igghDdAMwRTpflkK.txt  \n",
            " extracting: train/labels/000053_jpg.rf.CKbDRke2tRgXEsPhKzZJ.txt  \n",
            " extracting: train/labels/000053_jpg.rf.Co4eoGvBcpzq6J2zio2J.txt  \n",
            " extracting: train/labels/000053_jpg.rf.Vyoh6m3RFZ341XOzkuRb.txt  \n",
            " extracting: train/labels/000056_jpg.rf.0a4gr3SSIGzaWJGBa9CV.txt  \n",
            " extracting: train/labels/000056_jpg.rf.2CelKwCEU3z2K5CrcA6L.txt  \n",
            " extracting: train/labels/000056_jpg.rf.SL4Cfhzt7BshzL80dEvl.txt  \n",
            " extracting: train/labels/000058_jpg.rf.B9XHIcuwIYB89C2bjuc4.txt  \n",
            " extracting: train/labels/000058_jpg.rf.diJde8x4u6pbsFMSHzEZ.txt  \n",
            " extracting: train/labels/000058_jpg.rf.iPaT4SUlgXx5dh8R4jTG.txt  \n",
            " extracting: train/labels/000059_jpg.rf.43tEU54pUwvLHman2f2L.txt  \n",
            " extracting: train/labels/000059_jpg.rf.h4YNG9uB5YjBAZtJzCFJ.txt  \n",
            " extracting: train/labels/000059_jpg.rf.nHQY1dTBsNrCLwKgxMiR.txt  \n",
            " extracting: train/labels/000060_jpg.rf.Ezu21nMPvhWarOAhzhVN.txt  \n",
            " extracting: train/labels/000060_jpg.rf.Fo86oGpZklm537JRijIJ.txt  \n",
            " extracting: train/labels/000060_jpg.rf.Ph3mdJbBSVVUMV96KV2A.txt  \n",
            " extracting: train/labels/000061_jpg.rf.9rceQGgVUWOnoFRLEXe8.txt  \n",
            " extracting: train/labels/000061_jpg.rf.hn5AjDoonGQxmE1lnZIy.txt  \n",
            " extracting: train/labels/000061_jpg.rf.uNGIgLhOEV0cydH3ZwR6.txt  \n",
            " extracting: train/labels/000062_jpg.rf.XwAowGfpKTUWSJmVFaiA.txt  \n",
            " extracting: train/labels/000062_jpg.rf.fKhBTYmddvoY0FmhJKDP.txt  \n",
            " extracting: train/labels/000062_jpg.rf.uwLNGlHZySbja0WiKLiV.txt  \n",
            " extracting: train/labels/000064_jpg.rf.PlcuzIdUDG9YD2WjUSb8.txt  \n",
            " extracting: train/labels/000064_jpg.rf.StmrhCuB7nehk4zEDpky.txt  \n",
            " extracting: train/labels/000064_jpg.rf.ZztH6MoKg9va3fhkwvpT.txt  \n",
            " extracting: train/labels/000066_jpg.rf.4OG1QXxFH3N8qCudadHd.txt  \n",
            " extracting: train/labels/000066_jpg.rf.p8K3hXAOf74M6Zwc4hpX.txt  \n",
            " extracting: train/labels/000066_jpg.rf.xNp07mcDxrw2PKGm5ApN.txt  \n",
            " extracting: train/labels/000067_jpg.rf.AU9S8lcvCxzkNncyJEcW.txt  \n",
            " extracting: train/labels/000067_jpg.rf.phDbe78mDYxNjfxhw12u.txt  \n",
            " extracting: train/labels/000067_jpg.rf.x8bC5BrhxJAbivtlaoio.txt  \n",
            " extracting: train/labels/000068_jpg.rf.2a8QNKFhsevAVDGXayKX.txt  \n",
            " extracting: train/labels/000068_jpg.rf.gfP54GQ6ESCxzdhBYI1a.txt  \n",
            " extracting: train/labels/000068_jpg.rf.kt7IgkP9T9TCS38SOytV.txt  \n",
            " extracting: train/labels/000069_jpg.rf.Pc7hiToi0T6cRsmJH0OW.txt  \n",
            " extracting: train/labels/000069_jpg.rf.frJxgBn5VxdulzAyVivx.txt  \n",
            " extracting: train/labels/000069_jpg.rf.x3ceAocjuwAIANvuTaRy.txt  \n",
            " extracting: train/labels/000070_jpg.rf.8qdHFoKKBIAyrUQ3rpoU.txt  \n",
            " extracting: train/labels/000070_jpg.rf.DYa8vJbo4YaynXD9GmL2.txt  \n",
            " extracting: train/labels/000070_jpg.rf.JoHfNbdfZ8OdLn5ditWv.txt  \n",
            " extracting: train/labels/000072_jpg.rf.P4lshP7NUFWOBoPV5Tf9.txt  \n",
            " extracting: train/labels/000072_jpg.rf.jkyDVIc64m6QAlMW3hoD.txt  \n",
            " extracting: train/labels/000072_jpg.rf.qdicqt6kCH9rGNVEOpPh.txt  \n",
            " extracting: train/labels/000074_jpg.rf.2m3knj6pDI8IRWmxhGnE.txt  \n",
            " extracting: train/labels/000074_jpg.rf.CYrm6uUkUH8AGkObfRf7.txt  \n",
            " extracting: train/labels/000074_jpg.rf.WWphez97zt1EDabxVtZj.txt  \n",
            " extracting: train/labels/000075_jpg.rf.Kb5jECyf1XjFK60KnXDW.txt  \n",
            " extracting: train/labels/000075_jpg.rf.T1kKTwyKp5sPcoLklunT.txt  \n",
            " extracting: train/labels/000075_jpg.rf.uBDuTQ0sb5cO0CDjYwuP.txt  \n",
            " extracting: train/labels/000077_jpg.rf.QgiM7UiYM06PMHua94Bz.txt  \n",
            " extracting: train/labels/000077_jpg.rf.f2hRQ4wEoKAZ3DMqWjJC.txt  \n",
            " extracting: train/labels/000077_jpg.rf.kum6CWTGwuntrBG6jM4H.txt  \n",
            " extracting: train/labels/000078_jpg.rf.Am2M4dYnUUehPP5d2ng7.txt  \n",
            " extracting: train/labels/000078_jpg.rf.aJ4glOM4IuISluylbu4d.txt  \n",
            " extracting: train/labels/000078_jpg.rf.txjrIWpIWnYlG2yZU8oP.txt  \n",
            " extracting: train/labels/000079_jpg.rf.Kl8bcYNYSVIhhYfq1rCI.txt  \n",
            " extracting: train/labels/000079_jpg.rf.hY5tpB8hEjafbYmlPNNU.txt  \n",
            " extracting: train/labels/000079_jpg.rf.m6Af8XgalPfGHM9uWaHO.txt  \n",
            " extracting: train/labels/000080_jpg.rf.XJujCarCyuraqyGSoMgh.txt  \n",
            " extracting: train/labels/000080_jpg.rf.ZfXapbI53t3jmPkKIdaD.txt  \n",
            " extracting: train/labels/000080_jpg.rf.ddBfIzWIV6vhJt9EYtKG.txt  \n",
            " extracting: train/labels/000081_jpg.rf.auiCT6gKIL9VJWpesW4H.txt  \n",
            " extracting: train/labels/000081_jpg.rf.lXW06Ef5UDaODlUJI7hp.txt  \n",
            " extracting: train/labels/000081_jpg.rf.pXr6hIqs8MKLe2Sgjw0s.txt  \n",
            " extracting: train/labels/000082_jpg.rf.JApjBNvaNk6vp7rznKZz.txt  \n",
            " extracting: train/labels/000082_jpg.rf.YXz0obtreRGSz5Wx3Ey1.txt  \n",
            " extracting: train/labels/000082_jpg.rf.u4sgJAx8DpUVB63Y2RoK.txt  \n",
            " extracting: train/labels/000083_jpg.rf.i90UF3uqLUo0x1UUkDuh.txt  \n",
            " extracting: train/labels/000083_jpg.rf.kUwbglpTLRNBrTF4u9EQ.txt  \n",
            " extracting: train/labels/000083_jpg.rf.rHeuCMgPUHCzCG756dSW.txt  \n",
            " extracting: train/labels/000085_jpg.rf.HRqvRkJXUQxr85CNq4Yv.txt  \n",
            " extracting: train/labels/000085_jpg.rf.ojZTHjtQkhIdQZGtLAGI.txt  \n",
            " extracting: train/labels/000085_jpg.rf.p1vKiX1XZVeNzxz21xO0.txt  \n",
            " extracting: train/labels/000086_jpg.rf.CxPgEdd3mbV6evmaCp49.txt  \n",
            " extracting: train/labels/000086_jpg.rf.OpueTNVC1FnN1rGHc5v0.txt  \n",
            " extracting: train/labels/000086_jpg.rf.X1s7KaJvG0v6gkevWWWU.txt  \n",
            " extracting: train/labels/000087_jpg.rf.EgZkP3DUfudWAVGTl4lh.txt  \n",
            " extracting: train/labels/000087_jpg.rf.hH4SrRIge12i9f9R4U4J.txt  \n",
            " extracting: train/labels/000087_jpg.rf.nbDoL6ci6E9ACVmXcZHg.txt  \n",
            " extracting: train/labels/000089_jpg.rf.Ba6OX2YxVIZIZ1NQ7BPC.txt  \n",
            " extracting: train/labels/000089_jpg.rf.RZSak1vtx2CmU4i9bjYA.txt  \n",
            " extracting: train/labels/000089_jpg.rf.i8RDm0isVu7bkLWR6UMR.txt  \n",
            " extracting: train/labels/000090_jpg.rf.TydapqS4Mmh9dto5obLJ.txt  \n",
            " extracting: train/labels/000090_jpg.rf.UA9rk7GIjZYyTBHKkxOZ.txt  \n",
            " extracting: train/labels/000090_jpg.rf.VVyrrhRCtnezzTqzC9Cb.txt  \n",
            " extracting: train/labels/000092_jpg.rf.34crkSGa86TCRm3RE9nd.txt  \n",
            " extracting: train/labels/000092_jpg.rf.AtiTFVFe7pc7DD4oulad.txt  \n",
            " extracting: train/labels/000092_jpg.rf.W5NQzGcFgb0bCheLCuqv.txt  \n",
            " extracting: train/labels/000093_jpg.rf.MDerlMiMf0r5JC6rkGxS.txt  \n",
            " extracting: train/labels/000093_jpg.rf.VXRxCQKS5nYyA8Oh21cj.txt  \n",
            " extracting: train/labels/000093_jpg.rf.Vt4A9IytAtIJV3lXHzEV.txt  \n",
            " extracting: train/labels/000094_jpg.rf.EmnfIwqIRrEO4dCfjoty.txt  \n",
            " extracting: train/labels/000094_jpg.rf.XNS73rJJcgzh4GjxdBti.txt  \n",
            " extracting: train/labels/000094_jpg.rf.jeuuncHqmcnlnS1GODg6.txt  \n",
            " extracting: train/labels/000096_jpg.rf.5wfNybuHvYMOjE56dIGW.txt  \n",
            " extracting: train/labels/000096_jpg.rf.FDtlsfqlNYS8290694xq.txt  \n",
            " extracting: train/labels/000096_jpg.rf.ixWWVlafcwTHcJ1gJtKc.txt  \n",
            " extracting: train/labels/000100_jpg.rf.FNzjBw2Sw5uu59aao3Ah.txt  \n",
            " extracting: train/labels/000100_jpg.rf.XXrB6X48vPfpWlqSt6fp.txt  \n",
            " extracting: train/labels/000100_jpg.rf.pmGwo5ADKklyj1dvkPM2.txt  \n",
            "   creating: valid/\n",
            "   creating: valid/images/\n",
            " extracting: valid/images/000002_jpg.rf.NicXF4F7nReYj0bKynaH.jpg  \n",
            " extracting: valid/images/000003_jpg.rf.8hl12lZyrbycpW6t4KiM.jpg  \n",
            " extracting: valid/images/000004_jpg.rf.q3c7gQzetyUm0TBCNvk8.jpg  \n",
            " extracting: valid/images/000005_jpg.rf.MHXJTgoa2xli3qXjmrlg.jpg  \n",
            " extracting: valid/images/000006_jpg.rf.ohvJwFq5t1kZQ7evpR0q.jpg  \n",
            " extracting: valid/images/000009_jpg.rf.mPn6pJ6pJ2GJbgqlHK5F.jpg  \n",
            " extracting: valid/images/000010_jpg.rf.iMjp6wjjooSHqQvXI45C.jpg  \n",
            " extracting: valid/images/000015_jpg.rf.yBTSwenVCXA3laPLXhXr.jpg  \n",
            " extracting: valid/images/000043_jpg.rf.w2kpvQXMYTaO4tsF9ewx.jpg  \n",
            " extracting: valid/images/000046_jpg.rf.owzF6lIvdlou5KsZ5vnS.jpg  \n",
            " extracting: valid/images/000048_jpg.rf.U6ob0PJcHlrD4orH3s1a.jpg  \n",
            " extracting: valid/images/000055_jpg.rf.v6tyJvf6LqIEfJNHlETc.jpg  \n",
            " extracting: valid/images/000063_jpg.rf.VOyvQ3F2WqaeMdBkeyCU.jpg  \n",
            " extracting: valid/images/000065_jpg.rf.DJhXeyNuYrQFgmVZ71JM.jpg  \n",
            " extracting: valid/images/000071_jpg.rf.9rgI2u9lEcDm8kScx2at.jpg  \n",
            " extracting: valid/images/000088_jpg.rf.ueJUZRCWimELRIJGHjmw.jpg  \n",
            " extracting: valid/images/000091_jpg.rf.43tSHzfzyd9OGKQZONFm.jpg  \n",
            " extracting: valid/images/000099_jpg.rf.pX0cNFms5YGVuSdFagcJ.jpg  \n",
            " extracting: valid/images/000101_jpg.rf.QMsmPN1Qsa2LBoH8jBjc.jpg  \n",
            " extracting: valid/images/000102_jpg.rf.LsJ2PTeGTwBeTAfQHw6l.jpg  \n",
            "   creating: valid/labels/\n",
            " extracting: valid/labels/000002_jpg.rf.NicXF4F7nReYj0bKynaH.txt  \n",
            " extracting: valid/labels/000003_jpg.rf.8hl12lZyrbycpW6t4KiM.txt  \n",
            " extracting: valid/labels/000004_jpg.rf.q3c7gQzetyUm0TBCNvk8.txt  \n",
            " extracting: valid/labels/000005_jpg.rf.MHXJTgoa2xli3qXjmrlg.txt  \n",
            " extracting: valid/labels/000006_jpg.rf.ohvJwFq5t1kZQ7evpR0q.txt  \n",
            " extracting: valid/labels/000009_jpg.rf.mPn6pJ6pJ2GJbgqlHK5F.txt  \n",
            " extracting: valid/labels/000010_jpg.rf.iMjp6wjjooSHqQvXI45C.txt  \n",
            " extracting: valid/labels/000015_jpg.rf.yBTSwenVCXA3laPLXhXr.txt  \n",
            " extracting: valid/labels/000043_jpg.rf.w2kpvQXMYTaO4tsF9ewx.txt  \n",
            " extracting: valid/labels/000046_jpg.rf.owzF6lIvdlou5KsZ5vnS.txt  \n",
            " extracting: valid/labels/000048_jpg.rf.U6ob0PJcHlrD4orH3s1a.txt  \n",
            " extracting: valid/labels/000055_jpg.rf.v6tyJvf6LqIEfJNHlETc.txt  \n",
            " extracting: valid/labels/000063_jpg.rf.VOyvQ3F2WqaeMdBkeyCU.txt  \n",
            " extracting: valid/labels/000065_jpg.rf.DJhXeyNuYrQFgmVZ71JM.txt  \n",
            " extracting: valid/labels/000071_jpg.rf.9rgI2u9lEcDm8kScx2at.txt  \n",
            " extracting: valid/labels/000088_jpg.rf.ueJUZRCWimELRIJGHjmw.txt  \n",
            " extracting: valid/labels/000091_jpg.rf.43tSHzfzyd9OGKQZONFm.txt  \n",
            " extracting: valid/labels/000099_jpg.rf.pX0cNFms5YGVuSdFagcJ.txt  \n",
            " extracting: valid/labels/000101_jpg.rf.QMsmPN1Qsa2LBoH8jBjc.txt  \n",
            " extracting: valid/labels/000102_jpg.rf.LsJ2PTeGTwBeTAfQHw6l.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOg_uFYwrzSF"
      },
      "source": [
        "The export creates a YOLOv5 .yaml file called `data.yaml` specifying the location of a YOLOv5 `images` folder, a YOLOv5 `labels` folder, and information on our custom classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZ3DmmGQztJj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93df59a7-1107-4adc-9e27-bfb481b698f0"
      },
      "source": [
        "# this is the YAML file Roboflow wrote for us that we're loading into this notebook with our data\n",
        "%cat data.yaml ###For Project: Need to manually create the file with content inside. To edit the folder location of train..val ; nc (number of class) number of class, name of class. Copy and paste both image and text annotation into the respective folder"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: ../train/images\n",
            "val: ../valid/images\n",
            "test: ../test/images\n",
            "\n",
            "nc: 3\n",
            "names: ['head', 'helmet', 'person']\n",
            "\n",
            "roboflow:\n",
            "  workspace: project\n",
            "  project: hard-hat-sample-eoeka\n",
            "  version: 2\n",
            "  license: Public Domain\n",
            "  url: https://app.roboflow.com/project/hard-hat-sample-eoeka/2"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwJx-2NHsYxT"
      },
      "source": [
        "# Define Model Configuration and Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K982qQf7sHmJ"
      },
      "source": [
        "The smallest, fastest base model of YOLOv5 (YOLOv5s) is chosed. Other YOLOv5 models include: YOLOv5s / YOLOv5m / YOLOv5l / YOLOv5x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjUQUUktsX2g"
      },
      "source": [
        "We will write a yaml script that defines the parameters for our model like the number of classes, anchors, and each layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dOPn9wjOAwwK"
      },
      "source": [
        "# define number of classes based on YAML\n",
        "import yaml\n",
        "with open(\"data.yaml\", 'r') as stream:\n",
        "    num_classes = str(yaml.safe_load(stream)['nc'])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Rvt5wilnDyX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3c031f5-87e8-4326-94f6-ad0ad318fd39"
      },
      "source": [
        "#this is the model configuration we will use\n",
        "%cat /content/yolov5/models/yolov5s.yaml"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# YOLOv5 🚀 by Ultralytics, AGPL-3.0 license\n",
            "\n",
            "# Parameters\n",
            "nc: 80  # number of classes\n",
            "depth_multiple: 0.33  # model depth multiple\n",
            "width_multiple: 0.50  # layer channel multiple\n",
            "anchors:\n",
            "  - [10,13, 16,30, 33,23]  # P3/8\n",
            "  - [30,61, 62,45, 59,119]  # P4/16\n",
            "  - [116,90, 156,198, 373,326]  # P5/32\n",
            "\n",
            "# YOLOv5 v6.0 backbone\n",
            "backbone:\n",
            "  # [from, number, module, args]\n",
            "  [[-1, 1, Conv, [64, 6, 2, 2]],  # 0-P1/2\n",
            "   [-1, 1, Conv, [128, 3, 2]],  # 1-P2/4\n",
            "   [-1, 3, C3, [128]],\n",
            "   [-1, 1, Conv, [256, 3, 2]],  # 3-P3/8\n",
            "   [-1, 6, C3, [256]],\n",
            "   [-1, 1, Conv, [512, 3, 2]],  # 5-P4/16\n",
            "   [-1, 9, C3, [512]],\n",
            "   [-1, 1, Conv, [1024, 3, 2]],  # 7-P5/32\n",
            "   [-1, 3, C3, [1024]],\n",
            "   [-1, 1, SPPF, [1024, 5]],  # 9\n",
            "  ]\n",
            "\n",
            "# YOLOv5 v6.0 head\n",
            "head:\n",
            "  [[-1, 1, Conv, [512, 1, 1]],\n",
            "   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n",
            "   [[-1, 6], 1, Concat, [1]],  # cat backbone P4\n",
            "   [-1, 3, C3, [512, False]],  # 13\n",
            "\n",
            "   [-1, 1, Conv, [256, 1, 1]],\n",
            "   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n",
            "   [[-1, 4], 1, Concat, [1]],  # cat backbone P3\n",
            "   [-1, 3, C3, [256, False]],  # 17 (P3/8-small)\n",
            "\n",
            "   [-1, 1, Conv, [256, 3, 2]],\n",
            "   [[-1, 14], 1, Concat, [1]],  # cat head P4\n",
            "   [-1, 3, C3, [512, False]],  # 20 (P4/16-medium)\n",
            "\n",
            "   [-1, 1, Conv, [512, 3, 2]],\n",
            "   [[-1, 10], 1, Concat, [1]],  # cat head P5\n",
            "   [-1, 3, C3, [1024, False]],  # 23 (P5/32-large)\n",
            "\n",
            "   [[17, 20, 23], 1, Detect, [nc, anchors]],  # Detect(P3, P4, P5)\n",
            "  ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t14hhyqdmw6O"
      },
      "source": [
        "#customize iPython writefile so we can write variables\n",
        "from IPython.core.magic import register_line_cell_magic\n",
        "\n",
        "@register_line_cell_magic\n",
        "def writetemplate(line, cell):\n",
        "    with open(line, 'w') as f:\n",
        "        f.write(cell.format(**globals()))\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDxebz13RdRA"
      },
      "source": [
        "%%writetemplate /content/yolov5/models/custom_yolov5s.yaml\n",
        "\n",
        "# parameters\n",
        "nc: {num_classes}  # number of classes\n",
        "depth_multiple: 0.33  # model depth multiple\n",
        "width_multiple: 0.50  # layer channel multiple\n",
        "\n",
        "# anchors\n",
        "anchors:\n",
        "  - [10,13, 16,30, 33,23]  # P3/8\n",
        "  - [30,61, 62,45, 59,119]  # P4/16\n",
        "  - [116,90, 156,198, 373,326]  # P5/32\n",
        "\n",
        "# YOLOv5 backbone\n",
        "backbone:\n",
        "  # [from, number, module, args]\n",
        "  [[-1, 1, Focus, [64, 3]],  # 0-P1/2\n",
        "   [-1, 1, Conv, [128, 3, 2]],  # 1-P2/4\n",
        "   [-1, 3, BottleneckCSP, [128]],\n",
        "   [-1, 1, Conv, [256, 3, 2]],  # 3-P3/8\n",
        "   [-1, 9, BottleneckCSP, [256]],\n",
        "   [-1, 1, Conv, [512, 3, 2]],  # 5-P4/16\n",
        "   [-1, 9, BottleneckCSP, [512]],\n",
        "   [-1, 1, Conv, [1024, 3, 2]],  # 7-P5/32\n",
        "   [-1, 1, SPP, [1024, [5, 9, 13]]],\n",
        "   [-1, 3, BottleneckCSP, [1024, False]],  # 9\n",
        "  ]\n",
        "\n",
        "# YOLOv5 head\n",
        "head:\n",
        "  [[-1, 1, Conv, [512, 1, 1]],\n",
        "   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n",
        "   [[-1, 6], 1, Concat, [1]],  # cat backbone P4\n",
        "   [-1, 3, BottleneckCSP, [512, False]],  # 13\n",
        "\n",
        "   [-1, 1, Conv, [256, 1, 1]],\n",
        "   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n",
        "   [[-1, 4], 1, Concat, [1]],  # cat backbone P3\n",
        "   [-1, 3, BottleneckCSP, [256, False]],  # 17 (P3/8-small)\n",
        "\n",
        "   [-1, 1, Conv, [256, 3, 2]],\n",
        "   [[-1, 14], 1, Concat, [1]],  # cat head P4\n",
        "   [-1, 3, BottleneckCSP, [512, False]],  # 20 (P4/16-medium)\n",
        "\n",
        "   [-1, 1, Conv, [512, 3, 2]],\n",
        "   [[-1, 10], 1, Concat, [1]],  # cat head P5\n",
        "   [-1, 3, BottleneckCSP, [1024, False]],  # 23 (P5/32-large)\n",
        "\n",
        "   [[17, 20, 23], 1, Detect, [nc, anchors]],  # Detect(P3, P4, P5)\n",
        "  ]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUOiNLtMP5aG"
      },
      "source": [
        "# Train Custom YOLOv5 Detector\n",
        "\n",
        "With `data.yaml` and `custom_yolov5s.yaml` files ready, the training will begin with the following argument\n",
        "\n",
        "- **img:** define input image size\n",
        "- **batch:** determine batch size\n",
        "- **epochs:** define the number of training epochs. (Note: often, 3000+ are common here!)\n",
        "- **data:** set the path to our yaml file\n",
        "- **cfg:** specify our model configuration\n",
        "- **weights:** specify a custom path to weights.\n",
        "- **name:** result names\n",
        "- **nosave:** only save the final checkpoint\n",
        "- **cache:** cache images for faster training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NcFxRcFdJ_O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e66e21b8-6bec-4c54-b614-5be07753ea8b"
      },
      "source": [
        "# train yolov5s on custom data for 50 epochs\n",
        "# time its performance\n",
        "%%time\n",
        "%cd /content/yolov5/\n",
        "!python train.py --img 416 --batch 16 --epochs 50 --data '../data.yaml' --cfg ./models/custom_yolov5s.yaml --weights '' --name yolov5s_results  --cache"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/yolov5\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mweights=, cfg=./models/custom_yolov5s.yaml, data=../data.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=50, batch_size=16, imgsz=416, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=ram, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs/train, name=yolov5s_results, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
            "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 ✅\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m /content/requirements.txt not found, check failed.\n",
            "YOLOv5 🚀 v7.0-151-g3e14883 Python-3.9.16 torch-2.0.0+cu118 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
            "\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 🚀 in ClearML\n",
            "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
            "Downloading https://ultralytics.com/assets/Arial.ttf to /root/.config/Ultralytics/Arial.ttf...\n",
            "100% 755k/755k [00:00<00:00, 123MB/s]\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      3520  models.common.Focus                     [3, 32, 3]                    \n",
            "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
            "  2                -1  1     19904  models.common.BottleneckCSP             [64, 64, 1]                   \n",
            "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  4                -1  3    161152  models.common.BottleneckCSP             [128, 128, 3]                 \n",
            "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
            "  6                -1  3    641792  models.common.BottleneckCSP             [256, 256, 3]                 \n",
            "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
            "  8                -1  1    656896  models.common.SPP                       [512, 512, [5, 9, 13]]        \n",
            "  9                -1  1   1248768  models.common.BottleneckCSP             [512, 512, 1, False]          \n",
            " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  1    378624  models.common.BottleneckCSP             [512, 256, 1, False]          \n",
            " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  1     95104  models.common.BottleneckCSP             [256, 128, 1, False]          \n",
            " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  1    313088  models.common.BottleneckCSP             [256, 256, 1, False]          \n",
            " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  1   1248768  models.common.BottleneckCSP             [512, 512, 1, False]          \n",
            " 24      [17, 20, 23]  1     21576  models.yolo.Detect                      [3, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
            "custom_YOLOv5s summary: 233 layers, 7260488 parameters, 7260488 gradients\n",
            "\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 59 weight(decay=0.0), 70 weight(decay=0.0005), 62 bias\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/train/labels... 210 images, 0 backgrounds, 0 corrupt: 100% 210/210 [00:00<00:00, 649.92it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/train/labels.cache\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.1GB ram): 100% 210/210 [00:00<00:00, 250.17it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/valid/labels... 20 images, 0 backgrounds, 0 corrupt: 100% 20/20 [00:00<00:00, 570.13it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/valid/labels.cache\n",
            "\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.0GB ram): 100% 20/20 [00:00<00:00, 157.25it/s]\n",
            "\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m5.93 anchors/target, 1.000 Best Possible Recall (BPR). Current anchors are a good fit to dataset ✅\n",
            "Plotting labels to runs/train/yolov5s_results/labels.jpg... \n",
            "Image sizes 416 train, 416 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/train/yolov5s_results\u001b[0m\n",
            "Starting training for 50 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       0/49      1.72G     0.1096    0.04173    0.03832         18        416: 100% 14/14 [00:08<00:00,  1.68it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:01<00:00,  1.36s/it]\n",
            "                   all         20         65    0.00119     0.0519   0.000874   0.000296\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       1/49      1.72G     0.1087     0.0397    0.03745          4        416: 100% 14/14 [00:01<00:00,  7.43it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  3.90it/s]\n",
            "                   all         20         65   0.000877     0.0815   0.000701    0.00022\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       2/49      1.72G      0.105    0.04336    0.03678         12        416: 100% 14/14 [00:01<00:00,  7.71it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  3.83it/s]\n",
            "                   all         20         65    0.00068     0.0889   0.000724   0.000202\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       3/49      1.73G     0.1026    0.04273    0.03609         10        416: 100% 14/14 [00:02<00:00,  5.10it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  2.40it/s]\n",
            "                   all         20         65   0.000693     0.0889    0.00084   0.000181\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       4/49      1.73G        0.1    0.04349    0.03551          2        416: 100% 14/14 [00:02<00:00,  6.38it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  4.50it/s]\n",
            "                   all         20         65    0.00116      0.115   0.000724    0.00019\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       5/49      1.73G    0.09951     0.0464    0.03496         14        416: 100% 14/14 [00:01<00:00,  7.97it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  4.18it/s]\n",
            "                   all         20         65    0.00125      0.122   0.000784   0.000204\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       6/49      1.73G    0.09761      0.045    0.03435         20        416: 100% 14/14 [00:01<00:00,  7.90it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  4.78it/s]\n",
            "                   all         20         65    0.00157      0.159     0.0035   0.000874\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       7/49      1.73G    0.09577    0.04671    0.03361          2        416: 100% 14/14 [00:01<00:00,  7.22it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.20it/s]\n",
            "                   all         20         65   0.000296     0.0222   0.000165   4.27e-05\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       8/49      1.73G    0.09537    0.04711     0.0328         11        416: 100% 14/14 [00:02<00:00,  5.86it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  3.69it/s]\n",
            "                   all         20         65   0.000186     0.0148   0.000107   2.59e-05\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       9/49      1.73G    0.09541    0.04623    0.03215          7        416: 100% 14/14 [00:02<00:00,  5.46it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.97it/s]\n",
            "                   all         20         65   0.000453     0.0444   0.000308   7.94e-05\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      10/49      1.73G    0.09354    0.04284    0.03116          3        416: 100% 14/14 [00:01<00:00,  7.96it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  3.31it/s]\n",
            "                   all         20         65   0.000489     0.0444   0.000346   6.85e-05\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      11/49      1.73G    0.09267     0.0458    0.03022          7        416: 100% 14/14 [00:01<00:00,  7.98it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  4.13it/s]\n",
            "                   all         20         65    0.00105     0.0889   0.000791    0.00027\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      12/49      1.73G    0.09435    0.04627    0.02903          8        416: 100% 14/14 [00:01<00:00,  7.87it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.96it/s]\n",
            "                   all         20         65    0.00143      0.122    0.00191   0.000432\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      13/49      1.73G    0.09273    0.04853    0.02811          8        416: 100% 14/14 [00:01<00:00,  7.48it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.51it/s]\n",
            "                   all         20         65    0.00199      0.159    0.00287   0.000807\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      14/49      1.73G    0.09002    0.04701    0.02582          1        416: 100% 14/14 [00:03<00:00,  4.31it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  3.72it/s]\n",
            "                   all         20         65      0.739     0.0222     0.0208    0.00708\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      15/49      1.73G     0.0914    0.04622    0.02557          4        416: 100% 14/14 [00:01<00:00,  7.22it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  4.84it/s]\n",
            "                   all         20         65      0.721     0.0222     0.0312     0.0121\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      16/49      1.73G    0.08947    0.05077    0.02497         19        416: 100% 14/14 [00:01<00:00,  7.72it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.52it/s]\n",
            "                   all         20         65      0.725     0.0148     0.0187    0.00494\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      17/49      1.73G    0.08949    0.04937    0.02361         23        416: 100% 14/14 [00:01<00:00,  7.66it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.16it/s]\n",
            "                   all         20         65      0.704      0.037     0.0203    0.00326\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      18/49      1.73G    0.08878    0.04479    0.02394          8        416: 100% 14/14 [00:01<00:00,  8.05it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.17it/s]\n",
            "                   all         20         65      0.686     0.0222     0.0109    0.00322\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      19/49      1.73G    0.08836    0.04657    0.02241          6        416: 100% 14/14 [00:02<00:00,  6.12it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  2.69it/s]\n",
            "                   all         20         65      0.692     0.0963     0.0157    0.00372\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      20/49      1.73G    0.08679    0.04616    0.02247         16        416: 100% 14/14 [00:02<00:00,  5.46it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  4.82it/s]\n",
            "                   all         20         65      0.743     0.0444      0.044     0.0103\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      21/49      1.73G    0.08693    0.04462    0.02236          4        416: 100% 14/14 [00:01<00:00,  7.76it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  4.20it/s]\n",
            "                   all         20         65      0.751     0.0593     0.0532     0.0173\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      22/49      1.73G    0.08288    0.04708    0.02246          9        416: 100% 14/14 [00:01<00:00,  8.00it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  4.96it/s]\n",
            "                   all         20         65      0.753     0.0741     0.0776      0.017\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      23/49      1.73G    0.08198    0.04575    0.02428          8        416: 100% 14/14 [00:01<00:00,  8.00it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  4.41it/s]\n",
            "                   all         20         65      0.724      0.037     0.0187    0.00469\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      24/49      1.73G    0.08341    0.04719    0.02543          6        416: 100% 14/14 [00:01<00:00,  7.60it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  3.04it/s]\n",
            "                   all         20         65      0.739     0.0741     0.0735     0.0236\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      25/49      1.73G    0.08217    0.04721    0.01731         17        416: 100% 14/14 [00:02<00:00,  5.26it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  2.52it/s]\n",
            "                   all         20         65      0.774     0.0963      0.067     0.0156\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      26/49      1.73G    0.07939     0.0477    0.01998         19        416: 100% 14/14 [00:01<00:00,  8.14it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  3.65it/s]\n",
            "                   all         20         65      0.761     0.0593     0.0468     0.0175\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      27/49      1.73G    0.07969     0.0433    0.02039          2        416: 100% 14/14 [00:01<00:00,  7.71it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  3.91it/s]\n",
            "                   all         20         65      0.774      0.142      0.113     0.0334\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      28/49      1.73G    0.07952    0.03975     0.0228          1        416: 100% 14/14 [00:01<00:00,  8.02it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  4.04it/s]\n",
            "                   all         20         65      0.817      0.116      0.127      0.042\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      29/49      1.73G    0.07842    0.04685    0.02152          8        416: 100% 14/14 [00:01<00:00,  7.48it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  4.52it/s]\n",
            "                   all         20         65       0.77      0.124     0.0961     0.0324\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      30/49      1.73G    0.07819    0.04422    0.01998         14        416: 100% 14/14 [00:02<00:00,  5.07it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  2.74it/s]\n",
            "                   all         20         65      0.767      0.104     0.0724     0.0227\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      31/49      1.73G    0.07737    0.04814    0.01943         21        416: 100% 14/14 [00:02<00:00,  6.95it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  4.79it/s]\n",
            "                   all         20         65      0.404      0.126     0.0934     0.0245\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      32/49      1.73G    0.07466    0.04454     0.0204         12        416: 100% 14/14 [00:01<00:00,  7.96it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  4.10it/s]\n",
            "                   all         20         65      0.762      0.111      0.091     0.0285\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      33/49      1.73G     0.0744    0.04319    0.02104          9        416: 100% 14/14 [00:01<00:00,  8.20it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  3.68it/s]\n",
            "                   all         20         65      0.488      0.133      0.179     0.0558\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      34/49      1.73G    0.07423    0.04161     0.0184          9        416: 100% 14/14 [00:01<00:00,  8.19it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  3.82it/s]\n",
            "                   all         20         65      0.483      0.192      0.185     0.0608\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      35/49      1.73G    0.07218    0.03988    0.01974          9        416: 100% 14/14 [00:02<00:00,  5.62it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  2.44it/s]\n",
            "                   all         20         65      0.488      0.206      0.167     0.0627\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      36/49      1.73G    0.07366    0.04408    0.02155         14        416: 100% 14/14 [00:02<00:00,  5.97it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  3.62it/s]\n",
            "                   all         20         65      0.505      0.189      0.155     0.0521\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      37/49      1.73G    0.07053    0.04282    0.01793          9        416: 100% 14/14 [00:01<00:00,  7.81it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  4.44it/s]\n",
            "                   all         20         65      0.526      0.167      0.159     0.0593\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      38/49      1.73G     0.0703    0.04209    0.01864          8        416: 100% 14/14 [00:01<00:00,  8.01it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  4.56it/s]\n",
            "                   all         20         65      0.505      0.204      0.183     0.0588\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      39/49      1.73G    0.07034     0.0428    0.01708         13        416: 100% 14/14 [00:01<00:00,  7.62it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.42it/s]\n",
            "                   all         20         65      0.513      0.163      0.177     0.0619\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      40/49      1.73G     0.0691    0.04457    0.01648         26        416: 100% 14/14 [00:01<00:00,  7.06it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  3.36it/s]\n",
            "                   all         20         65      0.567      0.205      0.206     0.0737\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      41/49      1.73G    0.07014     0.0438    0.01766          9        416: 100% 14/14 [00:02<00:00,  4.79it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.11it/s]\n",
            "                   all         20         65      0.493      0.328      0.216     0.0742\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      42/49      1.73G    0.06892    0.04291    0.01612         12        416: 100% 14/14 [00:01<00:00,  7.93it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  4.57it/s]\n",
            "                   all         20         65      0.506      0.237      0.207     0.0701\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      43/49      1.73G    0.06885    0.04093    0.01729         13        416: 100% 14/14 [00:01<00:00,  7.76it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.14it/s]\n",
            "                   all         20         65      0.485      0.148      0.148     0.0511\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      44/49      1.73G    0.06893    0.04209    0.01579          4        416: 100% 14/14 [00:01<00:00,  7.77it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.33it/s]\n",
            "                   all         20         65      0.517      0.237      0.193     0.0641\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      45/49      1.73G    0.06903    0.04189    0.01583         10        416: 100% 14/14 [00:01<00:00,  7.69it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.46it/s]\n",
            "                   all         20         65      0.543       0.28      0.222     0.0706\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      46/49      1.73G    0.06994    0.04233    0.01613         29        416: 100% 14/14 [00:02<00:00,  5.01it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  3.18it/s]\n",
            "                   all         20         65      0.487      0.222      0.161     0.0515\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      47/49      1.73G    0.06836    0.04162    0.01597         19        416: 100% 14/14 [00:02<00:00,  6.88it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.27it/s]\n",
            "                   all         20         65      0.593      0.233      0.213      0.079\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      48/49      1.73G    0.06821    0.03832    0.01571          6        416: 100% 14/14 [00:01<00:00,  7.78it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.08it/s]\n",
            "                   all         20         65      0.557       0.33      0.235     0.0892\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      49/49      1.73G    0.06764    0.04305    0.01607         26        416: 100% 14/14 [00:01<00:00,  7.82it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.93it/s]\n",
            "                   all         20         65      0.593        0.3      0.234     0.0776\n",
            "\n",
            "50 epochs completed in 0.040 hours.\n",
            "Optimizer stripped from runs/train/yolov5s_results/weights/last.pt, 14.8MB\n",
            "Optimizer stripped from runs/train/yolov5s_results/weights/best.pt, 14.8MB\n",
            "\n",
            "Validating runs/train/yolov5s_results/weights/best.pt...\n",
            "Fusing layers... \n",
            "custom_YOLOv5s summary: 182 layers, 7251912 parameters, 0 gradients\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.37it/s]\n",
            "                   all         20         65      0.555       0.33      0.234     0.0892\n",
            "                  head         20         18      0.291      0.389      0.179      0.054\n",
            "                helmet         20         45      0.373        0.6      0.499      0.209\n",
            "                person         20          2          1          0     0.0237    0.00474\n",
            "Results saved to \u001b[1mruns/train/yolov5s_results\u001b[0m\n",
            "CPU times: user 1.79 s, sys: 229 ms, total: 2.02 s\n",
            "Wall time: 2min 51s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U51JB67M6gRW"
      },
      "source": [
        "During training, you want to be watching the mAP@0.5 to see how your detector is performing - see this post on [breaking down mAP](https://blog.roboflow.com/what-is-mean-average-precision-object-detection/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJVs_4zEeVbF"
      },
      "source": [
        "# Evaluate Custom YOLOv5 Detector Performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exPiw4oI6qtQ"
      },
      "source": [
        "Now that we have completed training, we can evaluate how well the training procedure performed by looking at the validation metrics. The training script will drop tensorboard logs in runs. We visualize those here:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KN5ghjE6ZWh"
      },
      "source": [
        "Training losses and performance metrics are saved to Tensorboard and also to a logfile defined above with the **--name** flag when we train. In our case, we named this `yolov5s_results`. (If given no name, it defaults to `results.txt`.) The results file is plotted as a png after training completes.\n",
        "\n",
        "Note from Glenn: Partially completed `results.txt` files can be plotted with `from utils.utils import plot_results; plot_results()`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uPq9mVgiBql"
      },
      "source": [
        "# Export Trained Weights for Future Inference\n",
        "\n",
        "Now that you have trained your custom detector, you can export the trained weights you have made here for inference on your device elsewhere"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YH4CTzDRh00g"
      },
      "source": [
        "DataFolder = \"/content/drive/MyDrive/SDAAI/Capstone-Project/Safety-Hat\"\n",
        "\n",
        "#Other working link: https://drive.google.com/drive/folders/1qCi19Hdp-Zj_91Kl2FGKL4BGPP0-REZm?usp=share_link"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1x_wg3VeiXMW"
      },
      "source": [
        "import os\n",
        "model = \"/content/drive/MyDrive/SDAAI/Capstone-Project/Safety-Hat/model\"\n",
        "if not os.path.exists(model):\n",
        "  os.makedirs(model)\n",
        "\n",
        "%cp /content/yolov5/runs/train/yolov5s_results/weights/best.pt $model"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLJJdt7Z6oPW"
      },
      "source": [
        "Copy output images for reference"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run inferrence with trained weights using torch API"
      ],
      "metadata": {
        "id": "W8JFrJ8yNTym"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.hub.load('.', 'custom', path=\"/content/drive/MyDrive/SDAAI/Capstone-Project/Safety-Hat/model/best.pt\", source='local') "
      ],
      "metadata": {
        "id": "g8gnpaIlNQda",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db279b04-26f6-4aae-e72d-98f67711d7a8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "YOLOv5 🚀 v7.0-151-g3e14883 Python-3.9.16 torch-2.0.0+cu118 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31m\u001b[1mrequirements:\u001b[0m /content/requirements.txt not found, check failed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fusing layers... \n",
            "custom_YOLOv5s summary: 182 layers, 7251912 parameters, 0 gradients\n",
            "Adding AutoShape... \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the neccesary libraries\n",
        "import torch\n",
        "\n",
        "# Load the Model\n",
        "model = torch.hub.load('.', 'custom', path=\"/content/drive/MyDrive/SDAAI/Capstone-Project/Safety-Hat/model/best.pt\", source='local') "
      ],
      "metadata": {
        "id": "MASuhexzbhOk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a48229a-36db-4707-bb70-8b1d966f069f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "YOLOv5 🚀 v7.0-151-g3e14883 Python-3.9.16 torch-2.0.0+cu118 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n",
            "Fusing layers... \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31m\u001b[1mrequirements:\u001b[0m /content/requirements.txt not found, check failed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "custom_YOLOv5s summary: 182 layers, 7251912 parameters, 0 gradients\n",
            "Adding AutoShape... \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def annotate_image(image):\n",
        "    \n",
        "    # perform prediction\n",
        "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    results = model(image_rgb) \n",
        "\n",
        "    counter = 0    \n",
        "    for i in range(len(results.pandas().xyxy[0].name)):\n",
        "      name = results.pandas().xyxy[0].name[i]\n",
        "      if name  in ['head','helmet','person']:\n",
        "        counter += 1\n",
        "\n",
        "        startX = int(results.pandas().xyxy[0].xmin[i])\n",
        "        startY = int(results.pandas().xyxy[0].ymin[i])\n",
        "        endX = int(results.pandas().xyxy[0].xmax[i])\n",
        "        endY = int(results.pandas().xyxy[0].ymax[i])\n",
        "        confidence = results.pandas().xyxy[0].confidence[i]\n",
        "        label = \"{}: {:.2f}%\".format(name, confidence * 100)\n",
        "        if confidence > 0.6:\n",
        "          cv2.rectangle(image, (startX, startY), (endX, endY),\n",
        "                  (255,0,0), 2)\n",
        "          y = startY - 15 if startY - 15 > 15 else startY + 15\n",
        "          cv2.putText(image, label, (startX, y),\n",
        "            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,0), 2)\n",
        "          cv2.putText(image, \"Number detected: \" +str(counter), (10, 20),\n",
        "            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,0), 2)\n",
        "    \n",
        "    return image    "
      ],
      "metadata": {
        "id": "GvSAQi95EEio"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Process the video file"
      ],
      "metadata": {
        "id": "xtO_-lU7c7R5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "video_in_file = DataFolder + \"/Testvideo.mp4\"\n",
        "video_out_file = DataFolder + \"/Testvideo_output.mp4\"\n",
        "\n",
        "print(\"[INFO] accessing video stream...\")\n",
        "v_out = None\n",
        "\n",
        "v_in = cv2.VideoCapture(video_in_file)\n",
        "total_frame = int(v_in.get(cv2.CAP_PROP_FRAME_COUNT ))\n",
        "\n",
        "for frame_no in tqdm(range(total_frame), desc=\"Processing Video...\"):\n",
        "\n",
        "  (grabbed, frame) = v_in.read()\n",
        "\n",
        "  # if the frame was not grabbed then we've reached the end of\n",
        "  # the video stream so exit the script\n",
        "  if not grabbed:\n",
        "      print(\"[INFO] no frame read from stream - exiting\")\n",
        "      break\n",
        "          \n",
        "  annotated_img = annotate_image(frame)\n",
        "      \n",
        "  # check if the video writer is None\n",
        "  if v_out is None:\n",
        "      # initialize our video writer\n",
        "      fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
        "      v_out = cv2.VideoWriter(video_out_file, fourcc, \n",
        "                  int(v_in.get(cv2.CAP_PROP_FPS)),\n",
        "                  (frame.shape[1], frame.shape[0]), True) \n",
        "\n",
        "  # write the output frame to disk\n",
        "  v_out.write(annotated_img)\n",
        "    \n",
        "# release the file pointers\n",
        "print(\"\\n[INFO] cleaning up...\")\n",
        "v_out.release()\n",
        "v_in.release()"
      ],
      "metadata": {
        "id": "hI4wgNuIc5In",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7991589f-542b-4ec1-9976-077be5583de3"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] accessing video stream...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Video...: 100%|██████████| 423/423 [00:25<00:00, 16.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[INFO] cleaning up...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Play the Testvideo_output.mp4 file."
      ],
      "metadata": {
        "id": "zJUjq0fldEYL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "video_mp4 = DataFolder + \"/Testvideo_output.mp4\"\n",
        "!ffmpeg -y -loglevel info -i $video_out_file -vf scale=640:480 $video_mp4"
      ],
      "metadata": {
        "id": "kRjySgqtdGdu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ce4e41c-067b-4bd6-da50-6724ac1a9d09"
      },
      "execution_count": 17,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ffmpeg version 4.2.7-0ubuntu0.1 Copyright (c) 2000-2022 the FFmpeg developers\n",
            "  built with gcc 9 (Ubuntu 9.4.0-1ubuntu1~20.04.1)\n",
            "  configuration: --prefix=/usr --extra-version=0ubuntu0.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-avresample --disable-filter=resample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librsvg --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-nvenc --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
            "  libavutil      56. 31.100 / 56. 31.100\n",
            "  libavcodec     58. 54.100 / 58. 54.100\n",
            "  libavformat    58. 29.100 / 58. 29.100\n",
            "  libavdevice    58.  8.100 / 58.  8.100\n",
            "  libavfilter     7. 57.100 /  7. 57.100\n",
            "  libavresample   4.  0.  0 /  4.  0.  0\n",
            "  libswscale      5.  5.100 /  5.  5.100\n",
            "  libswresample   3.  5.100 /  3.  5.100\n",
            "  libpostproc    55.  5.100 / 55.  5.100\n",
            "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from '/content/drive/MyDrive/SDAAI/Capstone-Project/Safety-Hat/Testvideo_output.mp4':\n",
            "  Metadata:\n",
            "    major_brand     : isom\n",
            "    minor_version   : 512\n",
            "    compatible_brands: isomiso2mp41\n",
            "    encoder         : Lavf59.27.100\n",
            "  Duration: 00:00:18.39, start: 0.000000, bitrate: 11622 kb/s\n",
            "    Stream #0:0(und): Video: mpeg4 (Simple Profile) (mp4v / 0x7634706D), yuv420p, 1920x1080 [SAR 1:1 DAR 16:9], 11621 kb/s, 23 fps, 23 tbr, 11776 tbn, 23 tbc (default)\n",
            "    Metadata:\n",
            "      handler_name    : VideoHandler\n",
            "\u001b[4;31mOutput /content/drive/MyDrive/SDAAI/Capstone-Project/Safety-Hat/Testvideo_output.mp4 same as Input #0 - exiting\n",
            "\u001b[0m\u001b[0;33mFFmpeg cannot edit existing files in-place.\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def show_local_mp4_video(file_name, width=640, height=480):\n",
        "  import io\n",
        "  import base64\n",
        "  from IPython.display import HTML\n",
        "  video_encoded = base64.b64encode(io.open(file_name, 'rb').read())\n",
        "  return HTML(data='''<video width=\"{0}\" height=\"{1}\" alt=\"test\" controls>\n",
        "                        <source src=\"data:video/mp4;base64,{2}\" type=\"video/mp4\" />\n",
        "                      </video>'''.format(width, height, video_encoded.decode('ascii')))"
      ],
      "metadata": {
        "id": "gIsE6kYSdJNu"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_local_mp4_video(video_mp4, width=640, height=480)"
      ],
      "metadata": {
        "id": "ZK6kwxVGdLtc"
      },
      "execution_count": 20,
      "outputs": []
    }
  ]
}